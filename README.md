[![made-with-datalad](https://www.datalad.org/badges/made_with.svg)](https://datalad.org)

# Visual Braille Silico (VBS)

This repository contains all the code and datasets related to the Visual Braille Silico, in which we tested how Deep Neural Networks process different alphabets and whether, after training, they replicate the results found in human behavioural learning and neural organization.  
This project is not published yet, but you can have an early look in Chapter 4 of my (Filippo's) [PhD thesis](https://dial.uclouvain.be/pr/boreal/object/boreal:305718).  

#### Contact
Author: [Filippo Cerpelloni](https://orcid.org/0000-0001-8070-5753)  
For any question, you can contact me at: filippo [dot] cerpelloni [at] gmail [dot] com


## The repository
The repository is organized as a IODA folder. Below a schematic view of the main subfolders and datasets, as well as their details

```bash
├── code
│   ├── activations
│   ├── letters
│   ├── lib
│   ├── network
│   ├── src
│   ├── stats
│   ├── stimuli
│   ├── tests
│   └── visualization
├── inputs
│   ├── cornet_weights
│   ├── datasets (GIN repository, access upon request)
│   ├── letters
│   └── words
└── outputs
    ├── figures
    ├── logs
    ├── results (GIN repository, access upon request)
    └── weights (GIN repository, access upon request)
```

- code folders: all the necessary code is stored here. Ideally it should lead you to replicate the whole experiment (although I wouldn't suggest training the networks unless on a cluster computer, it took ages). Given the size of the computations, there is no main script that runs the whole pipeline. The specifics of the code snippets are commented individually. 

- inputs: contains all weights, stimuli used as starting point for the experiment. 
    - cornet_weights contains the literate networks as used by Agrawal and Dehane (2024)
    - datasets (available on GIN G-node) has the stimuli used in training and testing of a network. 
    - letters stores the letters stimuli used in the early analyses of this project
    - words contains the starting words created through a Matlab script, used also in previous experiments 

- outputs: contains preprocessing outputs, analyses, figures. `results` and `weights` are stored on GIN given their size.


## DataLad datasets

⚠️ Warning ⚠️  
This repo is made with Datalad. It's not mandatory to use it, but it automatizes the retrieval of a lot of data. Alternatively, you can download the code and the datasets individually. My personal reccomendation is to try it, with a lot of patience. 
[Check the guide on how to use it](./how-to-use-me.md) and the instructions below (autogenerated)

This repository is a [DataLad](https://www.datalad.org/) dataset. It provides
fine-grained data access down to the level of individual files, and allows for
tracking future updates. In order to use this repository for data retrieval,
[DataLad](https://www.datalad.org/) is required. It is a free and open source
command line tool, available for all major operating systems, and builds up on
Git and [git-annex](https://git-annex.branchable.com/) to allow sharing,
synchronizing, and version controlling collections of large files. You can find
information on how to install DataLad at
[handbook.datalad.org/en/latest/intro/installation.html](http://handbook.datalad.org/en/latest/intro/installation.html).

### Get the dataset

A DataLad dataset can be `cloned` by running

```
datalad install <url>
```

Once a dataset is cloned, it is a light-weight directory on your local machine.
At this point, it contains only small metadata and information on the identity
of the files in the dataset, but not actual _content_ of the (sometimes large)
data files.

### Retrieve dataset content

After cloning a dataset, you can retrieve file contents by running

```
datalad get <path/to/directory/or/file>`
```

This command will trigger a download of the files, directories, or subdatasets
you have specified.

DataLad datasets can contain other datasets, so called _subdatasets_. If you
clone the top-level dataset, subdatasets do not yet contain metadata and
information on the identity of files, but appear to be empty directories. In
order to retrieve file availability metadata in subdatasets, run

```
datalad get -n <path/to/subdataset>
```

Afterwards, you can browse the retrieved metadata to find out about subdataset
contents, and retrieve individual files with `datalad get`. If you use
`datalad get <path/to/subdataset>`, all contents of the subdataset will be
downloaded at once.

### Stay up-to-date

DataLad datasets can be updated. The command `datalad update` will _fetch_
updates and store them on a different branch (by default
`remotes/origin/master`). Running

```
datalad update --merge
```

will _pull_ available updates and integrate them in one go.

### Find out what has been done

DataLad datasets contain their history in the `git log`. By running `git log`
(or a tool that displays Git history) in the dataset or on specific files, you
can find out what has been done to the dataset or to individual files by whom,
and when.

### More information

More information on DataLad and how to use it can be found in the DataLad
Handbook at
[handbook.datalad.org](http://handbook.datalad.org/en/latest/index.html). The
chapter "DataLad datasets" can help you to familiarize yourself with the concept
of a dataset.
